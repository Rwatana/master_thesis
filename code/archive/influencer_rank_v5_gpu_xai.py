import pandas as pd
import os
from tqdm import tqdm
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from torch.nn import GRU, Linear, ReLU, Tanh, Dropout
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
from sklearn.metrics import ndcg_score
import emoji
from copy import deepcopy # âœ… XAI: é †åˆ—é‡è¦åº¦ã®ãŸã‚ã«è¿½åŠ 
import matplotlib.pyplot as plt # âœ… XAI: å¯è¦–åŒ–ã®ãŸã‚ã«è¿½åŠ 
import seaborn as sns # âœ… XAI: å¯è¦–åŒ–ã®ãŸã‚ã«è¿½åŠ 

# --- 1. å®šæ•°å®šç¾© ---
PREPROCESSED_FILE = 'posts_2017.csv'
HASHTAGS_FILE = 'hashtags_2017.csv'
MENTIONS_FILE = 'output_mentions_all_parallel.csv'
INFLUENCERS_FILE = 'influencers.txt'
MODEL_SAVE_PATH = f'influencer_rank_model_{time.strftime("%Y%m%d")}_rich_features_2017_3rd_v5.pth'
FEATURE_NAMES_FILE = 'feature_names.txt' # âœ… XAI: ç‰¹å¾´é‡åã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«

# âœ… GPUå¯¾å¿œ (1): ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ‡ãƒã‚¤ã‚¹ã®å®šç¾©
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"--- ğŸ’» Using device: {DEVICE} ---")


# --- 2. ãƒ‡ãƒ¼ã‚¿æº–å‚™é–¢æ•° (ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚’è¿”ã™ã‚ˆã†å¤‰æ›´) ---
def prepare_graph_data(end_date, num_months=12, metric_numerator='likes', metric_denominator='posts'):
    """
    æŒ‡å®šã•ã‚ŒãŸçµ‚äº†æ—¥ã¾ã§ã®Nãƒ¶æœˆé–“ã®ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
    âœ… XAI: æˆ»ã‚Šå€¤ã« feature_columns ã‚’è¿½åŠ 
    """
    print(f"\nBuilding graph sequence for {num_months} months ending on {end_date.strftime('%Y-%m')}...")
    print(f"Using Engagement Metric: {metric_numerator} / {metric_denominator}")
    
    # --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
    df_posts = pd.read_csv(PREPROCESSED_FILE, parse_dates=['datetime'], low_memory=False)
    if 'comments' not in df_posts.columns: df_posts['comments'] = 0
    df_hashtags = pd.read_csv(HASHTAGS_FILE, header=0, low_memory=False)
    df_hashtags.rename(columns={'source': 'username', 'target': 'hashtag'}, inplace=True)
    df_mentions = pd.read_csv(MENTIONS_FILE, header=0, low_memory=False)
    df_mentions.rename(columns={'source': 'username', 'target': 'mention'}, inplace=True)
    with open(INFLUENCERS_FILE, 'r', encoding='utf-8') as f: lines = f.readlines()
    lines = [line for line in lines if '===' not in line]
    from io import StringIO
    df_influencers_master = pd.read_csv(StringIO("".join(lines)), sep='\t', dtype=str)
    df_influencers_master.rename(columns={'#Followers': 'followers', '#Followees': 'followees', '#Posts': 'posts', 'Username': 'username', 'Category': 'category'}, inplace=True)
    df_hashtags['datetime'] = pd.to_datetime(df_hashtags['timestamp'], unit='s', errors='coerce').dropna()
    df_mentions['datetime'] = pd.to_datetime(df_mentions['timestamp'], unit='s', errors='coerce').dropna()
    df_posts['month'] = df_posts['datetime'].dt.to_period('M').dt.start_time
    active_influencers_set = set(df_posts['username'].unique())
    print(f"Found {len(active_influencers_set):,} active influencers in {PREPROCESSED_FILE}.")
    df_influencers = df_influencers_master[df_influencers_master['username'].isin(active_influencers_set)].copy()
    influencer_set = set(df_influencers['username'].astype(str))
    all_hashtags = set(df_hashtags['hashtag'].astype(str))
    all_mentions = set(df_mentions['mention'].astype(str))
    all_nodes = sorted(list(influencer_set | all_hashtags | all_mentions))
    node_to_idx = {node: i for i, node in enumerate(all_nodes)}
    influencer_indices = [node_to_idx[inf] for inf in influencer_set if inf in node_to_idx]

    # --- ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° ---
    node_df = pd.DataFrame({'username': all_nodes})
    profile_features = pd.merge(node_df, df_influencers[['username', 'followers', 'followees', 'posts', 'category']], on='username', how='left')
    for col in ['followers', 'followees', 'posts']:
        profile_features[col] = pd.to_numeric(profile_features[col], errors='coerce').fillna(0)
    category_dummies = pd.get_dummies(profile_features['category'], prefix='cat', dummy_na=True)
    profile_features = pd.concat([profile_features, category_dummies], axis=1).drop(columns=['category'])
    node_df['type'] = 'other_user'
    node_df.loc[node_df['username'].isin(influencer_set), 'type'] = 'influencer'
    node_df.loc[node_df['username'].isin(all_hashtags), 'type'] = 'hashtag'
    node_type_dummies = pd.get_dummies(node_df['type'], prefix='type')
    static_features = pd.concat([profile_features, node_type_dummies], axis=1)
    df_posts['emoji_count'] = df_posts['caption'].astype(str).apply(emoji.emoji_count)
    df_posts.sort_values(by=['username', 'datetime'], inplace=True)
    df_posts['post_interval_sec'] = df_posts.groupby('username')['datetime'].diff().dt.total_seconds()
    post_categories = [f'post_cat_{i}' for i in range(10)]
    df_posts['post_category'] = np.random.choice(post_categories, size=len(df_posts))
    df_posts['is_ad'] = np.random.choice([0, 1], size=len(df_posts), p=[0.9, 0.1])
    dynamic_agg = df_posts.groupby(['username', 'month']).agg(
        monthly_post_count=('datetime', 'size'), avg_caption_length=('caption', lambda x: x.astype(str).str.len().mean()),
        avg_tag_count=('tag_count', 'mean'), avg_sentiment=('sentiment', 'mean'),
        avg_emoji_count=('emoji_count', 'mean'), avg_post_interval=('post_interval_sec', 'mean'),
        ad_rate=('is_ad', 'mean')).reset_index()
    post_category_rate = df_posts.groupby(['username', 'month'])['post_category'].value_counts(normalize=True).unstack(fill_value=0)
    post_category_rate.columns = [f'rate_{col}' for col in post_category_rate.columns]
    dynamic_features = pd.merge(dynamic_agg, post_category_rate, on=['username', 'month'], how='left')
    
    monthly_graphs = []
    start_date = end_date - pd.DateOffset(months=num_months-1)
    
    feature_columns = list(static_features.drop('username', axis=1).columns) + list(dynamic_features.drop(['username', 'month'], axis=1).columns) + ['feedback_rate']
    
    global FEATURE_DIM
    FEATURE_DIM = len(feature_columns)
    print(f"Total raw feature dimension: {FEATURE_DIM}") 

    for snapshot_date in tqdm(pd.date_range(start_date, end_date, freq='ME'), desc="Building monthly graphs"):
        snapshot_month = snapshot_date.to_period('M').start_time
        current_hashtags = df_hashtags[df_hashtags['datetime'] <= snapshot_date]
        current_mentions = df_mentions[df_mentions['datetime'] <= snapshot_date]
        edges_ht = [(node_to_idx[str(u)], node_to_idx[str(h)]) for u, h in zip(current_hashtags['username'], current_hashtags['hashtag']) if str(u) in node_to_idx and str(h) in node_to_idx]
        edges_mt = [(node_to_idx[str(u)], node_to_idx[str(m)]) for u, m in zip(current_mentions['username'], current_mentions['mention']) if str(u) in node_to_idx and str(m) in node_to_idx]
        if not edges_ht and not edges_mt: continue
        edge_index = torch.tensor(list(set(edges_ht + edges_mt)), dtype=torch.long).t().contiguous()
        
        current_dynamic = dynamic_features[dynamic_features['month'] == snapshot_month]
        snapshot_features = pd.merge(static_features, current_dynamic, on='username', how='left')
        snapshot_features['feedback_rate'] = 0.0
        snapshot_features = snapshot_features[feature_columns].fillna(0)
        
        x = torch.tensor(snapshot_features.astype(float).values, dtype=torch.float)        
        monthly_posts_period = df_posts[df_posts['datetime'].dt.to_period('M') == snapshot_date.to_period('M')]
        monthly_agg = monthly_posts_period.groupby('username').agg(
            total_likes=('likes', 'sum'), total_comments=('comments', 'sum'), post_count=('datetime', 'size')).reset_index()
        
        if metric_numerator == 'likes_and_comments': monthly_agg['numerator'] = monthly_agg['total_likes'] + monthly_agg['total_comments']
        else: monthly_agg['numerator'] = monthly_agg['total_likes']
            
        if metric_denominator == 'followers':
            monthly_agg['avg_engagement_per_post'] = (monthly_agg['numerator'] / monthly_agg['post_count']).where(monthly_agg['post_count'] > 0, 0)
            merged_data = pd.merge(monthly_agg, static_features[['username', 'followers']], on='username', how='left')
            merged_data['engagement'] = (merged_data['avg_engagement_per_post'] / merged_data['followers']).where(merged_data['followers'] > 0, 0)
        else:
            merged_data = monthly_agg
            merged_data['engagement'] = (merged_data['numerator'] / merged_data['post_count']).where(merged_data['post_count'] > 0, 0)
        
        engagement_data = pd.merge(pd.DataFrame({'username': all_nodes}), merged_data[['username', 'engagement']], on='username', how='left').fillna(0)
        y = torch.tensor(engagement_data['engagement'].values, dtype=torch.float).view(-1, 1)
        
        graph_data = Data(x=x, edge_index=edge_index, y=y) 
        monthly_graphs.append(graph_data)
        
    # âœ… XAI: feature_columns ã‚’è¿”ã™
    return monthly_graphs, influencer_indices, node_to_idx, feature_columns


# --- GCNEncoder (å¤‰æ›´ãªã—) ---
class GCNEncoder(nn.Module):
    def __init__(self, in_channels, hidden_channels, num_layers=2):
        super(GCNEncoder, self).__init__()
        self.num_layers = num_layers
        self.convs = nn.ModuleList([GCNConv(in_channels, hidden_channels)] + [GCNConv(hidden_channels, hidden_channels) for _ in range(num_layers - 1)])
    def forward(self, x, edge_index):
        layer_outputs = []
        for conv in self.convs:
            x = conv(x, edge_index).relu()
            layer_outputs.append(x)
        return torch.cat(layer_outputs, dim=1)

# --- AttentiveRNN (âœ… XAI: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®é‡ã¿ã‚’è¿”ã™ã‚ˆã†å¤‰æ›´) ---
class AttentiveRNN(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(AttentiveRNN, self).__init__()
        self.rnn = GRU(input_dim, hidden_dim, batch_first=True)
        self.attention_layer = Linear(hidden_dim, 1)
    def forward(self, sequence_of_embeddings):
        rnn_out, _ = self.rnn(sequence_of_embeddings)
        attention_scores = self.attention_layer(rnn_out).tanh()
        attention_weights = torch.softmax(attention_scores, dim=1)
        
        final_representation = torch.sum(rnn_out * attention_weights, dim=1)
        
        # âœ… XAI: æœ€çµ‚è¡¨ç¾ã¨ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®é‡ã¿ [Batch, Seq] ã‚’ä¸¡æ–¹è¿”ã™
        return final_representation, attention_weights.squeeze(-1)

# --- InfluencerRankModel (âœ… XAI: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®é‡ã¿ã‚’è¿”ã™ã‚ˆã†å¤‰æ›´) ---
class InfluencerRankModel(nn.Module):
    def __init__(self, feature_dim, gcn_dim, rnn_dim, num_gcn_layers=2, dropout_prob=0.5, projection_dim=128):
        super(InfluencerRankModel, self).__init__()
        print(f"\nInitializing Model:")
        print(f"  Raw Features (Input): {feature_dim}")
        print(f"  Projection Dim (GCN Input): {projection_dim}")
        print(f"  GCN Hidden Dim: {gcn_dim} (x{num_gcn_layers} layers -> Output: {gcn_dim * num_gcn_layers})")
        print(f"  RNN Hidden Dim (Predictor Input): {rnn_dim}")

        self.projection_layer = nn.Sequential(
            Linear(feature_dim, projection_dim),
            ReLU()
        )
        self.gcn_encoder = GCNEncoder(projection_dim, gcn_dim, num_gcn_layers)
        self.attentive_rnn = AttentiveRNN(gcn_dim * num_gcn_layers, rnn_dim)
        self.predictor = nn.Sequential(Linear(rnn_dim, 16), ReLU(), Dropout(dropout_prob), Linear(16, 1))

    # âœ… GPUå¯¾å¿œ (2): forwardãƒ¡ã‚½ãƒƒãƒ‰ã§ `device` ã‚’å—ã‘å–ã‚‹ã‚ˆã†ã«å¤‰æ›´
    def forward(self, graph_sequence, target_indices, device, debug_print=False):
        """
        ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹å…¨ä½“ã€‚
        âœ… XAI: äºˆæ¸¬ã‚¹ã‚³ã‚¢ã¨ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®é‡ã¿ã‚’è¿”ã™
        """
        if debug_print: 
            print(f"\n--- ğŸ› DEBUG: Model Forward Pass (BatchSize={len(target_indices)}) ---")
            print(f"Input: {len(graph_sequence)} graphs, {len(target_indices)} target indices. Target Device: {device}")

        gcn_inputs = []
        # --- 1. å°„å½±å±¤ (ã‚°ãƒ©ãƒ•ã”ã¨) ---
        for i, g in enumerate(graph_sequence):
            # âœ… GPUå¯¾å¿œ (3): ã‚°ãƒ©ãƒ•ã® 'x' ãƒ†ãƒ³ã‚½ãƒ«ã‚’GPUã«è»¢é€
            g_x = g.x.to(device) 
            if i == 0 and debug_print: print(f"[1] Projection Layer Input (g.x shape, T=0): {g_x.shape} (on {g_x.device})")
            projected_x = self.projection_layer(g_x)
            if i == 0 and debug_print: print(f"[1] Projection Layer Output (shape, T=0):      {projected_x.shape}")
            gcn_inputs.append(projected_x)

        # --- 2. GCNã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ (ã‚°ãƒ©ãƒ•ã”ã¨) ---
        sequence_embeddings_list = []
        for i, (g, projected_x) in enumerate(zip(graph_sequence, gcn_inputs)):
            # âœ… GPUå¯¾å¿œ (4): ã‚°ãƒ©ãƒ•ã® 'edge_index' ãƒ†ãƒ³ã‚½ãƒ«ã‚’GPUã«è»¢é€
            g_edge_index = g.edge_index.to(device)
            if i == 0 and debug_print: print(f"\n[2] GCN Encoder Input (projected_x, T=0): {projected_x.shape}")
            if i == 0 and debug_print: print(f"[2] GCN Encoder Input (edge_index, T=0):  {g_edge_index.shape} (on {g_edge_index.device})")
            gcn_out = self.gcn_encoder(projected_x, g_edge_index)
            if i == 0 and debug_print: print(f"[2] GCN Encoder Output (shape, T=0):      {gcn_out.shape}")
            sequence_embeddings_list.append(gcn_out)
        
        # [Seq_Len, Num_Nodes, GCN_Out_Feat]
        sequence_embeddings = torch.stack(sequence_embeddings_list)
        if debug_print: print(f"\n[3] Stacked GCN Embeddings (Seq, AllNodes, Feat): {sequence_embeddings.shape} (on {sequence_embeddings.device})")
        
        # --- 3. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé¸æŠ & è»¢ç½® ---
        # [Batch_Size, Seq_Len, GCN_Out_Feat]
        target_embeddings = sequence_embeddings[:, target_indices].permute(1, 0, 2)
        if debug_print: print(f"[3] Target Embeddings (Batch, Seq, Feat):        {target_embeddings.shape}")

        # --- 4. Attentive RNN ---
        if debug_print: print(f"\n[4] Attentive RNN Input (Batch, Seq, Feat):  {target_embeddings.shape}")
        # âœ… XAI: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®é‡ã¿ã‚‚å—ã‘å–ã‚‹
        final_representation, attention_weights = self.attentive_rnn(target_embeddings)
        if debug_print: print(f"[4] Attentive RNN Output (Batch, Feat): {final_representation.shape}")
        if debug_print: print(f"[4] Attention Weights (Batch, Seq): {attention_weights.shape}")

        # --- 5. äºˆæ¸¬å±¤ ---
        if debug_print: print(f"\n[5] Predictor Input (Batch, Feat):  {final_representation.shape}")
        predicted_scores = self.predictor(final_representation)
        if debug_print: print(f"[5] Predictor Output (Batch, 1): {predicted_scores.shape}")
        if debug_print: print(f"--- ğŸ› End Debug ---")

        # âœ… XAI: ã‚¹ã‚³ã‚¢ã¨ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®é‡ã¿ã‚’è¿”ã™
        return predicted_scores, attention_weights


# --- æå¤±é–¢æ•°ã¨è©•ä¾¡é–¢æ•° ---
class BatchedListwiseRankingLoss(nn.Module):
    def __init__(self):
        super(BatchedListwiseRankingLoss, self).__init__()
    def forward(self, pred_scores, true_scores):
        pred_probs = F.softmax(pred_scores, dim=1)
        true_probs = F.softmax(true_scores, dim=1)
        return -torch.sum(true_probs * torch.log(pred_probs + 1e-9), dim=1).mean()

class PointwiseRankingLoss(nn.Module):
    def __init__(self):
        super(PointwiseRankingLoss, self).__init__()
    def forward(self, pred_scores, true_scores):
        return F.mse_loss(pred_scores, true_scores)
        
def display_relevance_distribution(scores, title):
    scores_series = pd.Series(scores)
    relevance_series = scores_series.apply(assign_relevance_levels)
    counts = relevance_series.value_counts().sort_index()
    percentages = relevance_series.value_counts(normalize=True).sort_index() * 100
    dist_df = pd.DataFrame({'Relevance': counts.index, 'Count': counts.values, 'Percentage': percentages.values}).set_index('Relevance')
    dist_df = dist_df.reindex(range(6), fill_value=0)
    dist_df['Percentage'] = dist_df['Percentage'].map('{:.2f}%'.format)
    print(f"\n--- {title} ---")
    print(dist_df)

def assign_relevance_levels(engagement_rate):
    """ è«–æ–‡(Table 2)ã«åŸºã¥ãã€å˜ä¸€ã®ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ã‚’é–¢é€£ãƒ¬ãƒ™ãƒ«ã«å¤‰æ›ã™ã‚‹ """
    if engagement_rate >= 0.10: return 5
    if engagement_rate >= 0.07: return 4
    if engagement_rate >= 0.05: return 3
    if engagement_rate >= 0.03: return 2
    if engagement_rate >= 0.01: return 1
    return 0

def calculate_rbp(true_scores_in_predicted_order, p=0.95):
    rbp_score = 0
    max_score = true_scores_in_predicted_order.max()
    if max_score == 0: return 0.0
    normalized_scores = true_scores_in_predicted_order / max_score
    for i, relevance in enumerate(normalized_scores):
        rbp_score += (p ** i) * relevance
    return (1 - p) * rbp_score


# --- train_and_save_model (âœ… XAI: ç‰¹å¾´é‡åã‚’ä¿å­˜ã™ã‚‹ã‚ˆã†å¤‰æ›´) ---
def train_and_save_model():
    END_TO_END_TRAINING = False
    GCN_DIM = 128
    NUM_GCN_LAYERS = 2
    RNN_DIM = 64
    LEARNING_RATE = 0.001
    DROPOUT_PROB = 0.5
    NUM_EPOCHS = 200 # ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å¢—ã‚„ã™ (è«–æ–‡ã«åˆã‚ã›ã¦)
    LISTS_PER_BATCH = 1024 # è«–æ–‡ã®è¨­å®š
    LIST_SIZE = 10 # è«–æ–‡ã®è¨­å®š
    BATCH_SIZE = LISTS_PER_BATCH * LIST_SIZE
    METRIC_NUMERATOR = 'likes_and_comments'
    METRIC_DENOMINATOR = 'followers' # è«–æ–‡ã®å®šç¾© (Eq 1) ã«å³å¯†ã«åˆã‚ã›ã‚‹
    PROJECTION_DIM = 128

    print(f"--- Starting Training ---")
    start_time = time.time()
    
    latest_date = pd.to_datetime('2017-12-31')
    # âœ… XAI: feature_columns ã‚’å—ã‘å–ã‚‹
    monthly_graphs, influencer_indices, _, feature_columns = prepare_graph_data(
        end_date=latest_date, num_months=12, 
        metric_numerator=METRIC_NUMERATOR, metric_denominator=METRIC_DENOMINATOR
    )
    if not monthly_graphs:
        print("No graph data was created. Exiting.")
        return

    model = InfluencerRankModel(
        feature_dim=FEATURE_DIM, 
        gcn_dim=GCN_DIM, 
        rnn_dim=RNN_DIM, 
        num_gcn_layers=NUM_GCN_LAYERS, 
        dropout_prob=DROPOUT_PROB,
        projection_dim=PROJECTION_DIM
    )
    # âœ… GPUå¯¾å¿œ (5): ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ã—ãŸãƒ‡ãƒã‚¤ã‚¹(GPU)ã«è»¢é€
    model.to(DEVICE)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion_listwise = BatchedListwiseRankingLoss()
    criterion_pointwise = PointwiseRankingLoss() # MSELoss
    alpha = 1 # Listwiseã¨Pointwiseã®ãƒãƒ©ãƒ³ã‚¹ (èª¿æ•´å¯èƒ½)
    
    true_scores = monthly_graphs[-1].y[influencer_indices]
    # .cpu() ã¯ã€GPUãƒ†ãƒ³ã‚½ãƒ«ã‹ã‚‰Numpyé…åˆ—ã«å¤‰æ›ã™ã‚‹å‰ã«å¿…è¦
    display_relevance_distribution(true_scores.squeeze().cpu().numpy(), "ğŸ“Š Training Data Ground Truth Distribution")
    
    # DataLoader ã¯ CPU ä¸Šã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã‚¹ã‚³ã‚¢ã‚’ä¿æŒã—ã¾ã™
    dataset = TensorDataset(torch.tensor(influencer_indices, dtype=torch.long), true_scores)
    # pin_memory=True ã¯ã€CPUã‹ã‚‰GPUã¸ã®ãƒ‡ãƒ¼ã‚¿è»¢é€ã‚’é«˜é€ŸåŒ–ã™ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, pin_memory=True if DEVICE.type == 'cuda' else False)
    
    if not END_TO_END_TRAINING:
        # --- æˆ¦ç•¥1: 2æ®µéšå­¦ç¿’ ---
        print("\n--- Strategy: Two-Stage Learning (Fast) ---")
        model.projection_layer.eval()
        model.gcn_encoder.eval()
        
        with torch.no_grad():
            sequence_embeddings_list = []
            print("Running Projection + GCN Encoding (Two-Stage)...")
            debug_print_2stage = True 
            
            for i, g in enumerate(tqdm(monthly_graphs, desc="Projection + GCN Encoding")):
                # âœ… GPUå¯¾å¿œ (6): 2æ®µéšå­¦ç¿’ã®å‰è¨ˆç®—ã§ã‚‚ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«è»¢é€
                g_x = g.x.to(DEVICE)
                g_edge_index = g.edge_index.to(DEVICE)
                
                if i == 0 and debug_print_2stage: print(f"[1] Projection Input (T=0): {g_x.shape} (on {g_x.device})")
                projected_x = model.projection_layer(g_x)
                if i == 0 and debug_print_2stage: print(f"[1] Projection Output (T=0): {projected_x.shape}")
                
                if i == 0 and debug_print_2stage: print(f"[2] GCN Input (T=0): {projected_x.shape}")
                gcn_out = model.gcn_encoder(projected_x, g_edge_index)
                if i == 0 and debug_print_2stage: print(f"[2] GCN Output (T=0): {gcn_out.shape}")
                
                sequence_embeddings_list.append(gcn_out)
                
            # ã“ã®æ™‚ç‚¹ã§ sequence_embeddings ã¯ GPU ä¸Šã«ã‚ã‚‹
            sequence_embeddings = torch.stack(sequence_embeddings_list)
            if debug_print_2stage: print(f"[3] Stacked GCN Embeddings (Seq, AllNodes, Feat): {sequence_embeddings.shape} (on {sequence_embeddings.device})")

        model.attentive_rnn.train()
        model.predictor.train()
        
        print("\n--- ğŸ› DEBUG: Two-Stage RNN/Predictor (1 Batch) ---")
        batch_indices, _ = next(iter(dataloader))
        # batch_indices (CPU) ã‚’ä½¿ã£ã¦ sequence_embeddings (GPU) ã‹ã‚‰ã‚¹ãƒ©ã‚¤ã‚¹ã™ã‚‹
        debug_target_embeddings = sequence_embeddings[:, batch_indices].permute(1, 0, 2)
        print(f"[3] Target Embeddings (Batch, Seq, Feat): {debug_target_embeddings.shape} (on {debug_target_embeddings.device})")
        # âœ… XAI: æˆ»ã‚Šå€¤ã‚’2ã¤å—ã‘å–ã‚‹
        debug_rnn_out, debug_attn = model.attentive_rnn(debug_target_embeddings)
        print(f"[4] Attentive RNN Output (Batch, Feat): {debug_rnn_out.shape}")
        print(f"[4] Attention Weights (Batch, Seq): {debug_attn.shape}")
        debug_pred_out = model.predictor(debug_rnn_out)
        print(f"[5] Predictor Output (Batch, 1): {debug_pred_out.shape}")
        print("--- ğŸ› End Debug ---")

        for epoch in range(NUM_EPOCHS):
            total_loss = 0
            for batch_indices, batch_true_scores in tqdm(dataloader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS}"):
                # âœ… GPUå¯¾å¿œ (7): ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’GPUã«è»¢é€
                # (sequence_embeddings ã¯æ—¢ã«GPUä¸Šã«ã‚ã‚‹)
                batch_indices = batch_indices.to(DEVICE)
                batch_true_scores = batch_true_scores.to(DEVICE)
                
                optimizer.zero_grad()
                batch_sequence_embeddings = sequence_embeddings[:, batch_indices].permute(1, 0, 2)
                # âœ… XAI: æˆ»ã‚Šå€¤ã‚’2ã¤å—ã‘å–ã‚‹ (ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¯ä½¿ã‚ãªã„)
                final_user_representation, _ = model.attentive_rnn(batch_sequence_embeddings)
                predicted_scores = model.predictor(final_user_representation)
                
                predicted_scores_reshaped = predicted_scores.view(LISTS_PER_BATCH, LIST_SIZE)
                batch_true_scores_reshaped = batch_true_scores.view(LISTS_PER_BATCH, LIST_SIZE)
                
                loss_listwise = criterion_listwise(predicted_scores_reshaped, batch_true_scores_reshaped)
                loss_pointwise = criterion_pointwise(predicted_scores.squeeze(), batch_true_scores.squeeze())
                loss = loss_listwise + alpha * loss_pointwise
                
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            print(f"Epoch {epoch+1}/{NUM_EPOCHS}, Average Batch Loss: {total_loss / len(dataloader):.4f}")
    else:
        # --- æˆ¦ç•¥2: ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰å­¦ç¿’ ---
        print("\n--- Strategy: End-to-End Learning (Slow, High-Memory) ---")
        model.train() 
        
        batch_indices_debug, _ = next(iter(dataloader))
        print("\n--- ğŸ› DEBUG: End-to-End (1 Batch) ---")
        # âœ… GPUå¯¾å¿œ (8): model.forward ã« `device=DEVICE` ã‚’æ¸¡ã™
        # âœ… XAI: æˆ»ã‚Šå€¤ã‚’2ã¤å—ã‘å–ã‚‹
        _, _ = model(monthly_graphs, batch_indices_debug.to(DEVICE), device=DEVICE, debug_print=True) 
        print("--- ğŸ› End Debug ---")
            
        for epoch in range(NUM_EPOCHS):
            total_loss = 0
            for batch_indices, batch_true_scores in tqdm(dataloader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS}"):
                # âœ… GPUå¯¾å¿œ (7): ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’GPUã«è»¢é€
                batch_indices = batch_indices.to(DEVICE)
                batch_true_scores = batch_true_scores.to(DEVICE)
                
                optimizer.zero_grad()
                
                # âœ… GPUå¯¾å¿œ (8): model.forward ã« `device=DEVICE` ã‚’æ¸¡ã™
                # âœ… XAI: æˆ»ã‚Šå€¤ã‚’2ã¤å—ã‘å–ã‚‹ (ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¯ä½¿ã‚ãªã„)
                predicted_scores, _ = model(monthly_graphs, batch_indices, device=DEVICE, debug_print=False) 

                predicted_scores_reshaped = predicted_scores.view(LISTS_PER_BATCH, LIST_SIZE)
                batch_true_scores_reshaped = batch_true_scores.view(LISTS_PER_BATCH, LIST_SIZE)
                loss_listwise = criterion_listwise(predicted_scores_reshaped, batch_true_scores_reshaped)
                loss_pointwise = criterion_pointwise(predicted_scores.squeeze(), batch_true_scores.squeeze())
                loss = loss_listwise + alpha * loss_pointwise

                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            print(f"Epoch {epoch+1}/{NUM_EPOCHS}, Average Batch Loss: {total_loss / len(dataloader):.4f}")
    
    torch.save(model.state_dict(), MODEL_SAVE_PATH)
    
    # âœ… XAI: ç‰¹å¾´é‡åã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open(FEATURE_NAMES_FILE, 'w', encoding='utf-8') as f:
        for feature_name in feature_columns:
            f.write(f"{feature_name}\n")
    print(f"âœ… Feature names saved to '{FEATURE_NAMES_FILE}'")

    end_time = time.time()
    print("\n--- Training Complete ---")
    print(f"âœ… Model saved to '{MODEL_SAVE_PATH}'")
    print(f"Total time: {end_time - start_time:.2f} seconds")


# --- âœ… XAI: é †åˆ—é‡è¦åº¦ï¼ˆPermutation Importanceï¼‰ã®åˆ†æé–¢æ•° ---
def run_permutation_importance(model, base_graphs, target_indices, feature_names, ground_truth_graph, base_ndcg_100, device):
    """
    é †åˆ—é‡è¦åº¦ï¼ˆPermutation Importanceï¼‰ã‚’è¨ˆç®—ã—ã€è¡¨ç¤ºã™ã‚‹ã€‚
    NDCG@100 ã®ä½ä¸‹å¹…ã‚’é‡è¦åº¦ã‚¹ã‚³ã‚¢ã¨ã™ã‚‹ã€‚
    """
    print("\n" + "="*50)
    print("ğŸ”¬ D. FEATURE IMPORTANCE (PERMUTATION)")
    print("="*50)
    print(f"Calculating importance for {len(feature_names)} features...")
    print(f"Baseline NDCG@100: {base_ndcg_100:.4f}")
    
    model.eval()
    importances = {}
    
    # ---
    # âœ… FIX: ã“ã“ãŒä¿®æ­£ç‚¹ã§ã™ã€‚
    # assign_relevance_levelsãŒé…åˆ—å…¨ä½“ã«é©ç”¨ã•ã‚Œã‚¨ãƒ©ãƒ¼ã«ãªã£ã¦ã„ãŸã®ã‚’ä¿®æ­£ã€‚
    # pd.Series.apply() ã‚’ä½¿ã£ã¦ã€å„è¦ç´ ï¼ˆã‚¹ã‚³ã‚¢ï¼‰ã”ã¨ã«é–¢æ•°ã‚’é©ç”¨ã—ã¾ã™ã€‚
    # ---
    # 1. ã‚¹ã‚³ã‚¢ã®NumPyé…åˆ—ã‚’å–å¾—
    true_scores_numpy = ground_truth_graph.y[target_indices].squeeze().cpu().numpy()
    # 2. Pandas Seriesã«å¤‰æ›ã—ã€.apply()ã§å„è¦ç´ ã«é–¢æ•°ã‚’é©ç”¨
    true_relevance_series = pd.Series(true_scores_numpy).apply(assign_relevance_levels)
    # 3. ndcg_scoreãŒæœŸå¾…ã™ã‚‹ 2D-array å½¢å¼ã«å¤‰æ›
    true_relevance_for_ndcg = true_relevance_series.values.reshape(1, -1)

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’GPUãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ï¼ˆãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ç”¨ï¼‰
    target_indices_tensor = torch.tensor(target_indices, dtype=torch.long).to(device)

    for i, feature_name in enumerate(tqdm(feature_names, desc="Permutation Importance")):
        # ã‚°ãƒ©ãƒ•ã®ãƒªã‚¹ãƒˆã‚’ãƒ‡ã‚£ãƒ¼ãƒ—ã‚³ãƒ”ãƒ¼ã—ã¦ã€å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›´ã—ãªã„ã‚ˆã†ã«ã™ã‚‹
        shuffled_graphs = deepcopy(base_graphs)
        
        # --- iç•ªç›®ã®ç‰¹å¾´é‡ã‚’ã€å…¨ãƒãƒ¼ãƒ‰ãƒ»å…¨ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ« ---
        for g in shuffled_graphs:
            # g.x ã¯ [Num_Nodes, Num_Features]
            feature_column = g.x[:, i].clone()
            
            # ã“ã®ç‰¹å¾´é‡åˆ—ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ä¸¦ã³æ›¿ãˆã‚‹
            permuted_indices = torch.randperm(feature_column.shape[0])
            g.x[:, i] = feature_column[permuted_indices]
        
        # --- ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ãŸã‚°ãƒ©ãƒ•ã§æ¨è«–ã‚’å®Ÿè¡Œ ---
        with torch.no_grad():
            # æˆ»ã‚Šå€¤ã¯ (ã‚¹ã‚³ã‚¢, ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³)
            predicted_scores_shuffled, _ = model(shuffled_graphs, target_indices_tensor, device=device, debug_print=False)
        
        # --- æ–°ã—ã„NDCGã‚’è¨ˆç®— ---
        predicted_scores_for_ndcg = predicted_scores_shuffled.squeeze().cpu().numpy().reshape(1, -1)
        
        ndcg_k = 100
        if ndcg_k > len(target_indices):
            ndcg_k = len(target_indices)
            
        new_ndcg_100 = ndcg_score(true_relevance_for_ndcg, predicted_scores_for_ndcg, k=ndcg_k)
        
        # é‡è¦åº¦ = ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‹ã‚‰ã®ã‚¹ã‚³ã‚¢ä½ä¸‹å¹…
        importance_score = base_ndcg_100 - new_ndcg_100
        importances[feature_name] = importance_score

    # --- çµæœã‚’è¡¨ç¤º ---
    df_importance = pd.DataFrame(importances.items(), columns=['Feature', 'Importance (NDCG@100 Drop)'])
    df_importance = df_importance.sort_values(by='Importance (NDCG@100 Drop)', ascending=False)
    
    print("\n--- Top 15 Most Important Features ---")
    print(df_importance.head(15).to_string(index=False))
    
    print("\n--- Top 10 Least Important Features ---")
    print(df_importance.tail(10).to_string(index=False))
    
    return df_importance

# --- run_inference (âœ… XAI: åˆ†æã¨å¯è¦–åŒ–ã‚’è¿½åŠ ) ---
def run_inference():
    METRIC_NUMERATOR = 'likes_and_comments'
    METRIC_DENOMINATOR = 'followers'
    PROJECTION_DIM = 128
    
    print("--- ğŸ“ˆ Starting Inference Process ---")
    start_time = time.time()
    params = {'GCN_DIM': 128, 'NUM_GCN_LAYERS': 2, 'RNN_DIM': 64, 'DROPOUT_PROB': 0.5}

    # âœ… XAI: ç‰¹å¾´é‡åã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã®ã§ã€_ ã§å—ã‘ã‚‹
    latest_date = pd.to_datetime('2017-12-31')
    predict_graphs, predict_indices, node_to_idx, _ = prepare_graph_data(
        end_date=latest_date, num_months=12, 
        metric_numerator=METRIC_NUMERATOR, metric_denominator=METRIC_DENOMINATOR
    )
    
    # âœ… XAI: ç‰¹å¾´é‡åã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰
    try:
        with open(FEATURE_NAMES_FILE, 'r', encoding='utf-8') as f:
            feature_columns = [line.strip() for line in f]
        print(f"Successfully loaded {len(feature_columns)} feature names from '{FEATURE_NAMES_FILE}'.")
    except FileNotFoundError:
        print(f"Error: Feature file '{FEATURE_NAMES_FILE}' not found. Please run training first.")
        return

    model = InfluencerRankModel(
        feature_dim=FEATURE_DIM, # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
        gcn_dim=params['GCN_DIM'], 
        rnn_dim=params['RNN_DIM'], 
        num_gcn_layers=params['NUM_GCN_LAYERS'], 
        dropout_prob=params['DROPOUT_PROB'],
        projection_dim=PROJECTION_DIM
    )
    
    # âœ… GPUå¯¾å¿œ (9): ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å‰ã«ã€ã¾ãšGPUã«è»¢é€ã™ã‚‹
    model.to(DEVICE) 
    
    try:
        model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))
        print(f"Successfully loaded model from '{MODEL_SAVE_PATH}' (on {DEVICE})")
    except FileNotFoundError:
        print(f"Error: Model file not found at '{MODEL_SAVE_PATH}'. Please run training first.")
        return

    # äºˆæ¸¬ã«ä½¿ã†ã®ã¯11ãƒ¶æœˆåˆ†ã®ã‚°ãƒ©ãƒ• (T=1...11)
    inference_input_graphs = predict_graphs[:-1] 
    # æ­£è§£ãƒ©ãƒ™ãƒ«ã¯12ãƒ¶æœˆç›®ã®ã‚°ãƒ©ãƒ• (T=12)
    ground_truth_graph = predict_graphs[-1]

    model.eval()
    with torch.no_grad():
        print("\n---  BUG DEBUG: Inference ---")
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’Pythonãƒªã‚¹ãƒˆã‹ã‚‰Tensorã«å¤‰æ›
        # (forwardãƒ¡ã‚½ãƒƒãƒ‰ã¯å†…éƒ¨ã§ãƒªã‚¹ãƒˆã‚’å‡¦ç†ã§ãã‚‹ãŒã€æ˜ç¤ºçš„ã«Tensorã«ã—ã¦ã‚‚è‰¯ã„)
        # ã“ã“ã§ã¯å…ƒã®Pythonãƒªã‚¹ãƒˆ `predict_indices` ã‚’ãã®ã¾ã¾ä½¿ã†
        
        # âœ… GPUå¯¾å¿œ (10): æ¨è«–æ™‚ã‚‚ model.forward ã« `device=DEVICE` ã‚’æ¸¡ã™
        # predict_indices ã¯Pythonãƒªã‚¹ãƒˆãªã®ã§ã€ãã®ã¾ã¾æ¸¡ã—ã¦OK
        # âœ… XAI: äºˆæ¸¬ã‚¹ã‚³ã‚¢ã¨ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®é‡ã¿ã‚’å—ã‘å–ã‚‹
        predicted_scores, attention_weights = model(
            inference_input_graphs, predict_indices, device=DEVICE, debug_print=True
        )
        print("--- ğŸ› End Debug ---")

    
    idx_to_node = {i: node for node, i in node_to_idx.items()}
    influencer_usernames = [idx_to_node[idx] for idx in predict_indices]
    
    # âœ… GPUå¯¾å¿œ (11): çµæœã‚’Numpy/Pandasã§å‡¦ç†ã™ã‚‹å‰ã« .cpu() ã§CPUã«æˆ»ã™
    predicted_scores_cpu = predicted_scores.squeeze().cpu().numpy()
    true_scores_cpu = ground_truth_graph.y[predict_indices].squeeze().cpu().numpy()
    
    df_results = pd.DataFrame({
        'Username': influencer_usernames,
        'Predicted_Score': predicted_scores_cpu,
        'True_Score': true_scores_cpu
    })
    
    mae = (df_results['Predicted_Score'] - df_results['True_Score']).abs().mean()
    mse = ((df_results['Predicted_Score'] - df_results['True_Score']) ** 2).mean()
    rmse = np.sqrt(mse)
    
    # NDCGè¨ˆç®—ã®ãŸã‚ã«ã€äºˆæ¸¬ã‚¹ã‚³ã‚¢ã¨çœŸã®é–¢é€£æ€§ãƒ¬ãƒ™ãƒ«ã‚’æº–å‚™
    df_results['Relevance'] = df_results['True_Score'].apply(assign_relevance_levels)
    true_relevance = df_results['Relevance'].values.reshape(1, -1)
    predicted_scores_for_ndcg = df_results['Predicted_Score'].values.reshape(1, -1)
    
    ndcg_results = {}
    k_values = [1, 10, 50, 100, 200]
    for k in k_values:
        if k > len(df_results): continue
        ndcg_results[f'NDCG@{k}'] = ndcg_score(true_relevance, predicted_scores_for_ndcg, k=k)
    
    df_sorted_by_pred = df_results.sort_values(by='Predicted_Score', ascending=False)
    true_scores_in_pred_order = df_sorted_by_pred['True_Score'].values
    rbp_val = calculate_rbp(true_scores_in_pred_order, p=0.95)

    df_results['Predicted_Rank'] = df_results['Predicted_Score'].rank(ascending=False, method='first').astype(int)
    
    print("\nğŸ† --- Top 20 Predicted Influencers (with True Scores) --- ğŸ†")
    print(df_results.sort_values(by='Predicted_Rank')[['Username', 'Predicted_Rank', 'Predicted_Score', 'True_Score']].head(20).to_string(index=False))
    
    
    print("\n\n" + "="*50)
    print("ğŸ“Š MODEL PERFORMANCE EVALUATION")
    print("="*50)
    
    display_relevance_distribution(df_results['True_Score'], "ğŸ“ˆ Inference Data Ground Truth Distribution")
    display_relevance_distribution(df_results['Predicted_Score'], "ğŸ¤– Inference Data Predicted Distribution")

    print("\nğŸ¯ --- A. Prediction Accuracy Metrics (å€¤ã®æ­£ç¢ºã•) ---")
    print(f"    - **MAE (å¹³å‡çµ¶å¯¾èª¤å·®)**: {mae:.4f}")
    print(f"    - **RMSE (äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®)**: {rmse:.4f}")

    print("\nğŸ… --- B. Ranking Quality Metrics (é †åºã®æ­£ã—ã•) ---")
    print(f"    - **NDCG@K (æ­£è¦åŒ–å‰²å¼•ç´¯ç©åˆ©å¾—)**:")
    for k_str, score in ndcg_results.items():
        print(f"      - {k_str:<8}: {score:.4f}")

    print(f"\n    - **RBP (ãƒ©ãƒ³ã‚¯ãƒã‚¤ã‚¢ã‚¹é©åˆç‡)**: {rbp_val:.4f}")
    
    # --- âœ… XAI (1): ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å¯è¦–åŒ– ---
    print("\n" + "="*50)
    print("ğŸ¨ C. ATTENTION VISUALIZATION (Top 5)")
    print("="*50)
    # äºˆæ¸¬ãƒ©ãƒ³ã‚¯ä¸Šä½5åã®ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    top_5_indices_in_df = df_results.sort_values(by='Predicted_Rank').head(5).index
    
    # attention_weights ã¯ [Batch_Size, Seq_Len(11)]
    # df_results ã® index ãŒãã®ã¾ã¾ Batch_Size ã® index ã«å¯¾å¿œã™ã‚‹
    top_5_usernames = df_results.loc[top_5_indices_in_df, 'Username'].values
    top_5_attentions = attention_weights[top_5_indices_in_df].cpu().numpy()
    
    try:
        # attention_weights ã¯ 11ãƒ¶æœˆåˆ† (T=1...11)
        num_months_attention = top_5_attentions.shape[1]
        # è«–æ–‡ã®ã‚°ãƒ©ãƒ•(Fig 1)ã«åˆã‚ã›ã¦ Jan ã‹ã‚‰ Dec ã§ãƒ©ãƒ™ãƒ«ä»˜ã‘ (äºˆæ¸¬å¯¾è±¡ã¯Dec)
        month_labels = pd.date_range(start=latest_date - pd.DateOffset(months=num_months_attention), periods=num_months_attention, freq='M').strftime('%b')
        
        df_attention = pd.DataFrame(top_5_attentions, index=top_5_usernames, columns=month_labels)
        
        plt.figure(figsize=(12, 4))
        sns.heatmap(df_attention, annot=True, fmt=".3f", cmap="viridis", linewidths=.5)
        plt.title("Attention Weights for Top 5 Predicted Influencers (When Predicting Dec)")
        plt.xlabel("Input Month (T=1 to T=11)")
        plt.ylabel("Influencer")
        plt.yticks(rotation=0)
        img_path = "attention_heatmap.png"
        plt.savefig(img_path)
        print(f"âœ… Attention heatmap saved to '{img_path}'")
        plt.close()

    except ImportError:
        print("âš ï¸ Matplotlib/Seaborn not found. Skipping heatmap visualization.")
        print("Top 5 Attentions (Raw):")
        for i, user in enumerate(top_5_usernames):
            print(f"  - {user}: {np.round(top_5_attentions[i], 3)}")
    
    # --- âœ… XAI (2): é †åˆ—é‡è¦åº¦ã®å®Ÿè¡Œ ---
    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®NDCG@100ã‚¹ã‚³ã‚¢ã‚’å–å¾—
    base_ndcg_100 = ndcg_results.get('NDCG@100')
    if base_ndcg_100 is None:
        # 100äººæœªæº€ã®å ´åˆã€è¨ˆç®—å¯èƒ½ãªæœ€å¤§ã®Kã§ä»£ç”¨
        max_k = max([k for k in k_values if k <= len(df_results)])
        base_ndcg_100 = ndcg_results.get(f'NDCG@{max_k}', 0.0)

    # é †åˆ—é‡è¦åº¦ã‚’è¨ˆç®—ãƒ»è¡¨ç¤º
    _ = run_permutation_importance(
        model=model,
        base_graphs=inference_input_graphs,
        target_indices=predict_indices,
        feature_names=feature_columns,
        ground_truth_graph=ground_truth_graph,
        base_ndcg_100=base_ndcg_100,
        device=DEVICE
    )
    
    end_time = time.time()
    print(f"\nTotal inference time: {end_time - start_time:.2f} seconds")

# --- ä¹±æ•°ã‚·ãƒ¼ãƒ‰è¨­å®šé–¢æ•° ---
def set_seed(seed_value=42):
    np.random.seed(seed_value) 
    torch.manual_seed(seed_value)
    if torch.cuda.is_available(): 
        torch.cuda.manual_seed_all(seed_value)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

# --- mainãƒ–ãƒ­ãƒƒã‚¯ ---
if __name__ == '__main__':
    set_seed(42) 
    train_and_save_model() # è¨“ç·´ã¨ç‰¹å¾´é‡åã®ä¿å­˜
    run_inference()          # æ¨è«–ã¨XAIåˆ†æã®å®Ÿè¡Œ