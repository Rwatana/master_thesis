import pandas as pd
import os
from tqdm import tqdm
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from torch.nn import GRU, Linear, ReLU, Tanh, Dropout
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
from sklearn.metrics import ndcg_score

# --- 1. å®šæ•°å®šç¾© ---
PREPROCESSED_FILE = 'preprocessed_posts_with_metadata.csv'
HASHTAGS_FILE = 'output_hashtags_all_parallel.csv'
MENTIONS_FILE = 'output_mentions_all_parallel.csv'
INFLUENCERS_FILE = 'influencers.txt'
MODEL_SAVE_PATH = f'influencer_rank_model_{time.strftime("%Y%m%d")}_metric.pth'

def prepare_graph_data(end_date, num_months=12, metric_numerator='likes', metric_denominator='posts'):
    """
    æŒ‡å®šã•ã‚ŒãŸçµ‚äº†æ—¥ã¾ã§ã®Nãƒ¶æœˆé–“ã®ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
    ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’é¸æŠå¯èƒ½ã«ã™ã‚‹ã€‚
    
    Args:
        end_date (pd.Timestamp): ã‚°ãƒ©ãƒ•æ§‹ç¯‰ã®çµ‚äº†æ—¥
        num_months (int): æ§‹ç¯‰ã™ã‚‹æœˆæ•°
        metric_numerator (str): 'likes' ã¾ãŸã¯ 'likes_and_comments'
        metric_denominator (str): 'posts' ã¾ãŸã¯ 'followers'
    """
    print(f"\nBuilding graph sequence for {num_months} months ending on {end_date.strftime('%Y-%m')}...")
    print(f"Using Engagement Metric: {metric_numerator} / {metric_denominator}")
    
    # --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
    df_posts = pd.read_csv(PREPROCESSED_FILE, parse_dates=['datetime'], low_memory=False)
    if 'comments' not in df_posts.columns:
        print("Warning: 'comments' column not found in df_posts. Defaulting to 0.")
        df_posts['comments'] = 0
        
    df_hashtags = pd.read_csv(HASHTAGS_FILE, header=0, low_memory=False)
    df_mentions = pd.read_csv(MENTIONS_FILE, header=0, low_memory=False)
    
    df_hashtags.rename(columns={'source': 'username', 'target': 'hashtag'}, inplace=True)
    df_mentions.rename(columns={'source': 'username', 'target': 'mention'}, inplace=True)
    with open(INFLUENCERS_FILE, 'r', encoding='utf-8') as f: lines = f.readlines()
    lines = [line for line in lines if '===' not in line]
    from io import StringIO
    df_influencers = pd.read_csv(StringIO("".join(lines)), sep='\t', dtype=str)
    df_influencers.rename(columns={'#Followers': 'followers', '#Followees': 'followees', '#Posts': 'posts'}, inplace=True)
    
    df_hashtags['datetime'] = pd.to_datetime(df_hashtags['timestamp'], unit='s', errors='coerce').dropna()
    df_mentions['datetime'] = pd.to_datetime(df_mentions['timestamp'], unit='s', errors='coerce').dropna()
    df_posts['month'] = df_posts['datetime'].dt.to_period('M').dt.start_time

    # --- ãƒãƒ¼ãƒ‰ã®æº–å‚™ ---
    influencer_set = set(df_influencers['Username'].astype(str))
    all_hashtags = set(df_hashtags['hashtag'].astype(str))
    all_mentions = set(df_mentions['mention'].astype(str))
    all_nodes = sorted(list(influencer_set | all_hashtags | all_mentions))
    node_to_idx = {node: i for i, node in enumerate(all_nodes)}
    influencer_indices = [node_to_idx[inf] for inf in influencer_set if inf in node_to_idx]

    # --- ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° ---
    static_features = pd.merge(pd.DataFrame({'Username': all_nodes}),
                               df_influencers[['Username', 'followers', 'followees', 'posts']],
                               on='Username', how='left').fillna(0)
    for col in ['followers', 'followees', 'posts']:
        static_features[col] = pd.to_numeric(static_features[col], errors='coerce').fillna(0)

    dynamic_features = df_posts.groupby(['username', 'month']).agg(
        monthly_post_count=('datetime', 'size'),
        avg_caption_length=('caption', lambda x: x.astype(str).str.len().mean()),
        avg_tag_count=('tag_count', 'mean'),
        avg_sentiment=('sentiment', 'mean')).reset_index()

    # --- ã‚°ãƒ©ãƒ•æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰ ---
    monthly_graphs = []
    start_date = end_date - pd.DateOffset(months=num_months-1)
    for snapshot_date in tqdm(pd.date_range(start_date, end_date, freq='ME'), desc="Building monthly graphs"):
        snapshot_month = snapshot_date.to_period('M').start_time
        current_hashtags = df_hashtags[df_hashtags['datetime'] <= snapshot_date]
        current_mentions = df_mentions[df_mentions['datetime'] <= snapshot_date]
        edges_ht = [(node_to_idx[str(u)], node_to_idx[str(h)]) for u, h in zip(current_hashtags['username'], current_hashtags['hashtag']) if str(u) in node_to_idx and str(h) in node_to_idx]
        edges_mt = [(node_to_idx[str(u)], node_to_idx[str(m)]) for u, m in zip(current_mentions['username'], current_mentions['mention']) if str(u) in node_to_idx and str(m) in node_to_idx]
        if not edges_ht and not edges_mt: continue
        edge_index = torch.tensor(list(set(edges_ht + edges_mt)), dtype=torch.long).t().contiguous()
        
        # --- x (ç‰¹å¾´é‡) ã®ä½œæˆ ---
        current_dynamic = dynamic_features[dynamic_features['month'] == snapshot_month]
        snapshot_features = pd.merge(static_features, current_dynamic, left_on='Username', right_on='username', how='left').fillna(0)
        feature_columns = ['followers', 'followees', 'posts', 'monthly_post_count', 'avg_caption_length', 'avg_tag_count', 'avg_sentiment']
        x = torch.tensor(snapshot_features[feature_columns].values, dtype=torch.float)
        
        # --- y (æ­£è§£ãƒ©ãƒ™ãƒ«) ã®ä½œæˆ ---
        monthly_posts_period = df_posts[df_posts['datetime'].dt.to_period('M') == snapshot_date.to_period('M')]
        
        monthly_agg = monthly_posts_period.groupby('username').agg(
            total_likes=('likes', 'sum'),
            total_comments=('comments', 'sum'),
            post_count=('datetime', 'size')
        ).reset_index()
        monthly_agg.rename(columns={'username': 'Username'}, inplace=True)
        
        if metric_numerator == 'likes_and_comments':
            monthly_agg['numerator'] = monthly_agg['total_likes'] + monthly_agg['total_comments']
        else:
            monthly_agg['numerator'] = monthly_agg['total_likes']
            
        # âœ…âœ…âœ… ã“ã“ã‹ã‚‰ãŒä¿®æ­£ç®‡æ‰€ âœ…âœ…âœ…
        if metric_denominator == 'followers':
            # è«–æ–‡ã®å®šç¾© E = (avg_likes) / followers ã«å¾“ã†
            # 1. ã¾ãšã€æœˆé–“ã®ã€Œå¹³å‡ã€ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚’è¨ˆç®— (åˆè¨ˆã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ / æŠ•ç¨¿æ•°)
            monthly_agg['avg_engagement_per_post'] = 0.0
            post_count_mask = monthly_agg['post_count'] > 0
            monthly_agg.loc[post_count_mask, 'avg_engagement_per_post'] = monthly_agg.loc[post_count_mask, 'numerator'] / monthly_agg.loc[post_count_mask, 'post_count']
            
            # 2. æ¬¡ã«ã€ãã®å¹³å‡å€¤ã‚’ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ã§å‰²ã‚‹
            merged_data = pd.merge(monthly_agg, static_features[['Username', 'followers']], on='Username', how='left')
            merged_data['engagement'] = 0.0
            followers_mask = merged_data['followers'] > 0
            # 0é™¤ç®—ã‚’é¿ã‘ã‚‹
            merged_data.loc[followers_mask, 'engagement'] = merged_data.loc[followers_mask, 'avg_engagement_per_post'] / merged_data.loc[followers_mask, 'followers']
            
        else: # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 'posts'
            # ã“ã¡ã‚‰ã¯ã€Œ1æŠ•ç¨¿ã‚ãŸã‚Šã®å¹³å‡ã€ãªã®ã§ã€å…ƒã®è¨ˆç®—ã§æ­£ã—ã„
            merged_data = monthly_agg
            merged_data['engagement'] = 0.0
            mask = merged_data['post_count'] > 0
            merged_data.loc[mask, 'engagement'] = merged_data.loc[mask, 'numerator'] / merged_data.loc[mask, 'post_count']
        
        engagement_data = pd.merge(pd.DataFrame({'Username': all_nodes}), merged_data[['Username', 'engagement']], on='Username', how='left').fillna(0)
        y = torch.tensor(engagement_data['engagement'].values, dtype=torch.float).view(-1, 1)
        
        graph_data = Data(x=x, edge_index=edge_index, y=y)
        monthly_graphs.append(graph_data)
        
    return monthly_graphs, influencer_indices, node_to_idx

# --- 3. ãƒ¢ãƒ‡ãƒ«å®šç¾© (å¤‰æ›´ãªã—) ---
class GCNEncoder(nn.Module):
    def __init__(self, in_channels, hidden_channels, num_layers=2):
        super(GCNEncoder, self).__init__()
        self.num_layers = num_layers
        self.convs = nn.ModuleList([GCNConv(in_channels, hidden_channels)] + [GCNConv(hidden_channels, hidden_channels) for _ in range(num_layers - 1)])
    def forward(self, x, edge_index):
        layer_outputs = []
        for conv in self.convs:
            x = conv(x, edge_index).relu()
            layer_outputs.append(x)
        return torch.cat(layer_outputs, dim=1)

class AttentiveRNN(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(AttentiveRNN, self).__init__()
        self.rnn = GRU(input_dim, hidden_dim, batch_first=True)
        self.attention_layer = Linear(hidden_dim, 1)
    def forward(self, sequence_of_embeddings):
        rnn_out, _ = self.rnn(sequence_of_embeddings)
        attention_scores = self.attention_layer(rnn_out).tanh()
        attention_weights = torch.softmax(attention_scores, dim=1)
        return torch.sum(rnn_out * attention_weights, dim=1)

class InfluencerRankModel(nn.Module):
    def __init__(self, feature_dim, gcn_dim, rnn_dim, num_gcn_layers=2, dropout_prob=0.5):
        super(InfluencerRankModel, self).__init__()
        self.gcn_encoder = GCNEncoder(feature_dim, gcn_dim, num_gcn_layers)
        self.attentive_rnn = AttentiveRNN(gcn_dim * num_gcn_layers, rnn_dim)
        #TODO ReLUæ´»æ€§åŒ–é–¢æ•°ã‚’è¿½åŠ ã—ãŸã‹ã‚‰ä¿®æ­£ã®å¯èƒ½æ€§ã‚ã‚‹ã‹ã‚‚
        self.predictor = nn.Sequential(Linear(rnn_dim, 16), ReLU(), Dropout(dropout_prob), Linear(16, 1), ReLU())

class BatchedListwiseRankingLoss(nn.Module):
    def __init__(self):
        super(BatchedListwiseRankingLoss, self).__init__()
    def forward(self, pred_scores, true_scores):
        pred_probs = F.softmax(pred_scores, dim=1)
        true_probs = F.softmax(true_scores, dim=1)
        return -torch.sum(true_probs * torch.log(pred_probs + 1e-9), dim=1).mean()

# --- 4. å­¦ç¿’ãƒ»æ¨è«–é–¢æ•° ---

# âœ… å…ƒã®mainé–¢æ•°ã‚’å­¦ç¿’å°‚ç”¨ã®é–¢æ•°ã¨ã—ã¦åå‰å¤‰æ›´ (ä¸­èº«ã¯å¤‰æ›´ãªã—)
def train_and_save_model():
    """ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã•ã›ã€é‡ã¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹å…ƒã®ã‚³ãƒ¼ãƒ‰"""
    END_TO_END_TRAINING = False
    GCN_DIM = 128
    NUM_GCN_LAYERS = 2
    RNN_DIM = 64
    LEARNING_RATE = 0.001
    DROPOUT_PROB = 0.5
    NUM_EPOCHS = 20
    LISTS_PER_BATCH = 1024
    LIST_SIZE = 10
    BATCH_SIZE = LISTS_PER_BATCH * LIST_SIZE
    # åˆ†å­: 'likes' ã¾ãŸã¯ 'likes_and_comments'
    METRIC_NUMERATOR = 'likes_and_comments'
    # åˆ†æ¯: 'posts' ã¾ãŸã¯ 'followers'
    METRIC_DENOMINATOR = 'followers'

    print(f"--- Starting Training ---")
    print(f"Mode: {'End-to-End' if END_TO_END_TRAINING else 'Two-Stage'}")
    start_time = time.time()

    df_posts = pd.read_csv(PREPROCESSED_FILE, parse_dates=['datetime'], low_memory=False)
    # latest_date = sorted(df_posts['datetime'].dt.to_period('M').dt.start_time.unique())[-1]
    latest_date = pd.to_datetime('2017-12-31')
    monthly_graphs, influencer_indices, _ = prepare_graph_data(end_date=latest_date, num_months=12, metric_numerator=METRIC_NUMERATOR, metric_denominator=METRIC_DENOMINATOR)
    if not monthly_graphs:
        print("No graph data was created. Exiting.")
        return

    model = InfluencerRankModel(feature_dim=7, gcn_dim=GCN_DIM, rnn_dim=RNN_DIM, num_gcn_layers=NUM_GCN_LAYERS, dropout_prob=DROPOUT_PROB)
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = BatchedListwiseRankingLoss()
    
    true_scores = monthly_graphs[-1].y[influencer_indices]
    display_relevance_distribution(
        true_scores.squeeze().cpu().numpy(), 
        "ğŸ“Š Training Data Ground Truth Distribution"
    )
    dataset = TensorDataset(torch.tensor(influencer_indices, dtype=torch.long), true_scores)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
    
    if not END_TO_END_TRAINING:
        print("\n--- Strategy: Two-Stage Learning (Fast) ---")
        model.gcn_encoder.eval()
        with torch.no_grad():
            sequence_embeddings = torch.stack([model.gcn_encoder(g.x, g.edge_index) for g in tqdm(monthly_graphs, desc="GCN Encoding")])
        model.attentive_rnn.train()
        model.predictor.train()
        for epoch in range(NUM_EPOCHS):
            total_loss = 0
            for batch_indices, batch_true_scores in tqdm(dataloader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS}"):
                optimizer.zero_grad()
                batch_sequence_embeddings = sequence_embeddings[:, batch_indices].permute(1, 0, 2)
                final_user_representation = model.attentive_rnn(batch_sequence_embeddings)
                predicted_scores = model.predictor(final_user_representation)
                predicted_scores_reshaped = predicted_scores.view(LISTS_PER_BATCH, LIST_SIZE)
                batch_true_scores_reshaped = batch_true_scores.view(LISTS_PER_BATCH, LIST_SIZE)
                loss = criterion(predicted_scores_reshaped, batch_true_scores_reshaped)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            print(f"Epoch {epoch+1}/{NUM_EPOCHS}, Average Batch Loss: {total_loss / len(dataloader):.4f}")
    else:
        print("\n--- Strategy: End-to-End Learning (Slow, High-Memory) ---")
        model.train()
        for epoch in range(NUM_EPOCHS):
            print(f"Epoch {epoch+1}/{NUM_EPOCHS}: Performing GCN forward pass for this epoch...")
            sequence_embeddings = torch.stack([model.gcn_encoder(g.x, g.edge_index) for g in monthly_graphs])
            total_loss = 0
            for batch_indices, batch_true_scores in tqdm(dataloader, desc=f"Training Batches"):
                optimizer.zero_grad()
                batch_sequence_embeddings = sequence_embeddings[:, batch_indices].permute(1, 0, 2)
                final_user_representation = model.attentive_rnn(batch_sequence_embeddings)
                predicted_scores = model.predictor(final_user_representation)
                predicted_scores_reshaped = predicted_scores.view(LISTS_PER_BATCH, LIST_SIZE)
                batch_true_scores_reshaped = batch_true_scores.view(LISTS_PER_BATCH, LIST_SIZE)
                loss = criterion(predicted_scores_reshaped, batch_true_scores_reshaped)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            print(f"Epoch {epoch+1}/{NUM_EPOCHS}, Average Batch Loss: {total_loss / len(dataloader):.4f}")

    torch.save(model.state_dict(), MODEL_SAVE_PATH)
    end_time = time.time()
    print("\n--- Training Complete ---")
    print(f"âœ… Model saved to '{MODEL_SAVE_PATH}'")
    print(f"Total time: {end_time - start_time:.2f} seconds")


# âœ…âœ…âœ… --- æ–°ã—ãè¿½åŠ ã—ãŸãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° --- âœ…âœ…âœ…
def display_relevance_distribution(scores, title):
    """
    ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ã®ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€
    Table 2ã«åŸºã¥ã„ãŸé–¢é€£æ€§ãƒ¬ãƒ™ãƒ«ã®åˆ†å¸ƒã‚’è¡¨ç¤ºã™ã‚‹ã€‚
    """
    # Relevance Engagement rate E(Â·)
    # 5: E(Â·) â‰¥ 0.10
    # 4: 0.10 > E(Â·) â‰¥ 0.07
    # 3: 0.07 > E(Â·) â‰¥ 0.05
    # 2: 0.05 > E(Â·) â‰¥ 0.03
    # 1: 0.03 > E(Â·) â‰¥ 0.01
    # 0: 0.01 > E(Â·)
    
    # ã‚¹ã‚³ã‚¢ã‚’ pandas Series ã«å¤‰æ›ã—ã¦å‡¦ç†
    scores_series = pd.Series(scores)
    relevance_series = scores_series.apply(assign_relevance_levels)
    
    counts = relevance_series.value_counts().sort_index()
    percentages = relevance_series.value_counts(normalize=True).sort_index() * 100
    
    # åˆ†å¸ƒã‚’ã¾ã¨ã‚ãŸDataFrameã‚’ä½œæˆ
    dist_df = pd.DataFrame({
        'Relevance': counts.index,
        'Count': counts.values,
        'Percentage': percentages.values
    }).set_index('Relevance')
    
    # å­˜åœ¨ã—ãªã„ãƒ¬ãƒ™ãƒ«ã‚‚è¡¨ç¤ºã™ã‚‹ãŸã‚ã« reindex
    dist_df = dist_df.reindex(range(6), fill_value=0)
    dist_df['Percentage'] = dist_df['Percentage'].map('{:.2f}%'.format)

    print(f"\n--- {title} ---")
    print(dist_df)

# --- è«–æ–‡ã®åŸºæº–(Table 2)ã«åŸºã¥ã„ã¦é–¢é€£æ€§ãƒ¬ãƒ™ãƒ«ã‚’å‰²ã‚Šå½“ã¦ã‚‹é–¢æ•° ---
def assign_relevance_levels(engagement_rate):
    """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ã‚’0ã‹ã‚‰5ã®é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã«å¤‰æ›ã™ã‚‹"""
    if engagement_rate >= 0.10: return 5
    if engagement_rate >= 0.07: return 4
    if engagement_rate >= 0.05: return 3
    if engagement_rate >= 0.03: return 2
    if engagement_rate >= 0.01: return 1
    return 0

# --- Rank-Biased Precision (RBP) ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•° ---
def calculate_rbp(true_scores_in_predicted_order, p=0.95):
    """
    äºˆæ¸¬é †ã«ä¸¦ã¹ãŸå®Ÿéš›ã®ã‚¹ã‚³ã‚¢ãƒªã‚¹ãƒˆã‹ã‚‰RBPã‚’è¨ˆç®—ã™ã‚‹ã€‚
    pã¯persistenceï¼ˆæŒç¶šæ€§ï¼‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚
    """
    rbp_score = 0
    max_score = true_scores_in_predicted_order.max()
    if max_score == 0: return 0.0
    
    normalized_scores = true_scores_in_predicted_order / max_score
    
    for i, relevance in enumerate(normalized_scores):
        rbp_score += (p ** i) * relevance
        
    return (1 - p) * rbp_score


# âœ…âœ…âœ… --- NDCG@Kã®è¨ˆç®—ã‚’è¿½åŠ ã—ãŸæ¨è«–é–¢æ•° --- âœ…âœ…âœ…
def run_inference():
    """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã§æ¨è«–ã‚’è¡Œã„ã€å„ç¨®è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ã™ã‚‹"""
    METRIC_NUMERATOR = 'likes_and_comments'
    # åˆ†æ¯: 'posts' ã¾ãŸã¯ 'followers'
    METRIC_DENOMINATOR = 'followers'
    
    print("--- ğŸ“ˆ Starting Inference Process ---")
    start_time = time.time()
    params = {'GCN_DIM': 128, 'NUM_GCN_LAYERS': 2, 'RNN_DIM': 64, 'DROPOUT_PROB': 0.5}

    # 1. ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã¨é‡ã¿ã®ãƒ­ãƒ¼ãƒ‰
    model = InfluencerRankModel(feature_dim=7, gcn_dim=params['GCN_DIM'], rnn_dim=params['RNN_DIM'], num_gcn_layers=params['NUM_GCN_LAYERS'], dropout_prob=params['DROPOUT_PROB'])
    try:
        model.load_state_dict(torch.load(MODEL_SAVE_PATH))
        print(f"Successfully loaded model from '{MODEL_SAVE_PATH}'")
    except FileNotFoundError:
        print(f"Error: Model file not found at '{MODEL_SAVE_PATH}'.")
        print("Please run the training process first by calling train_and_save_model().")
        return

    # 2. æ¨è«–ç”¨ãƒ‡ãƒ¼ã‚¿ã¨æ­£è§£ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    df_posts = pd.read_csv(PREPROCESSED_FILE, parse_dates=['datetime'], low_memory=False)
    # latest_date = sorted(df_posts['datetime'].dt.to_period('M').dt.start_time.unique())[-1]
    latest_date = pd.to_datetime('2017-12-31')

    predict_graphs, predict_indices, node_to_idx = prepare_graph_data(
        end_date=latest_date, 
        num_months=12,
        metric_numerator=METRIC_NUMERATOR,
        metric_denominator=METRIC_DENOMINATOR
    )
    inference_input_graphs = predict_graphs[:-1]
    ground_truth_graph = predict_graphs[-1]

    # 3. äºˆæ¸¬ã®å®Ÿè¡Œ
    model.eval()
    with torch.no_grad():
        sequence_embeddings = torch.stack([model.gcn_encoder(g.x, g.edge_index) for g in tqdm(inference_input_graphs, desc="GCN Encoding for Inference")])
        influencer_embeddings = sequence_embeddings[:, predict_indices].permute(1, 0, 2)
        final_representation = model.attentive_rnn(influencer_embeddings)
        predicted_scores = model.predictor(final_representation)

    # 4. çµæœã®é›†è¨ˆ
    idx_to_node = {i: node for node, i in node_to_idx.items()}
    influencer_usernames = [idx_to_node[idx] for idx in predict_indices]
    true_scores = ground_truth_graph.y[predict_indices]
    
    df_results = pd.DataFrame({
        'Username': influencer_usernames,
        'Predicted_Score': predicted_scores.squeeze().cpu().numpy(),
        'True_Score': true_scores.squeeze().cpu().numpy()
    })
    
    # 5. è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
    
    # --- 5.1 MAE/MSE (äºˆæ¸¬å€¤ã®æ­£ç¢ºã•) ---
    mae = (df_results['Predicted_Score'] - df_results['True_Score']).abs().mean()
    mse = ((df_results['Predicted_Score'] - df_results['True_Score']) ** 2).mean()
    rmse = np.sqrt(mse)

    # --- 5.2 NDCG@K (ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®é †åºè©•ä¾¡) --- âœ… å¤‰æ›´ç®‡æ‰€
    df_results['Relevance'] = df_results['True_Score'].apply(assign_relevance_levels)
    true_relevance = df_results['Relevance'].values.reshape(1, -1)
    predicted_scores_for_ndcg = df_results['Predicted_Score'].values.reshape(1, -1)
    
    ndcg_results = {}
    k_values = [1, 10, 50, 100, 200]
    for k in k_values:
        # ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã®ç·æ•°ã‚ˆã‚ŠKãŒå¤§ãã„å ´åˆã¯è¨ˆç®—ã—ãªã„
        if k > len(df_results):
            continue
        ndcg_results[f'NDCG@{k}'] = ndcg_score(true_relevance, predicted_scores_for_ndcg, k=k)
    
    # --- 5.3 RBP (ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®é †åºè©•ä¾¡) ---
    df_sorted_by_pred = df_results.sort_values(by='Predicted_Score', ascending=False)
    true_scores_in_pred_order = df_sorted_by_pred['True_Score'].values
    rbp_val = calculate_rbp(true_scores_in_pred_order, p=0.95)

    # 6. çµæœã®è¡¨ç¤º
    df_results['Predicted_Rank'] = df_results['Predicted_Score'].rank(ascending=False, method='first').astype(int)
    
    print("\nğŸ† --- Top 20 Predicted Influencers (with True Scores) --- ğŸ†")
    print(df_results.sort_values(by='Predicted_Rank')[['Username', 'Predicted_Score', 'True_Score']].head(20).to_string(index=False))
    
    print("\n\n" + "="*50)
    print("ğŸ“Š MODEL PERFORMANCE EVALUATION")
    print("="*50)
    
    # âœ…âœ…âœ… åˆ†å¸ƒè¡¨ç¤ºã‚’è¿½åŠ  âœ…âœ…âœ…
    # --- 6.1 åˆ†å¸ƒã®è¡¨ç¤º ---
    # æ¨è«–æ™‚ã®æ­£è§£ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒ
    display_relevance_distribution(df_results['True_Score'], "ğŸ“ˆ Inference Data Ground Truth Distribution")
    # ãƒ¢ãƒ‡ãƒ«ãŒäºˆæ¸¬ã—ãŸã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ
    display_relevance_distribution(df_results['Predicted_Score'], "ğŸ¤– Inference Data Predicted Distribution")

    print("\nğŸ¯ --- A. Prediction Accuracy Metrics (å€¤ã®æ­£ç¢ºã•) ---")
    print(f"   - **MAE (å¹³å‡çµ¶å¯¾èª¤å·®)**: {mae:.4f}")
    print(f"     (äºˆæ¸¬ãŒå¹³å‡ã—ã¦ã©ã‚Œãã‚‰ã„å¤–ã‚Œã¦ã„ã‚‹ã‹)")
    print(f"   - **RMSE (äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®)**: {rmse:.4f}")
    print(f"     (å¤§ããªå¤–ã‚Œã‚’ã‚ˆã‚Šé‡è¦–ã—ãŸèª¤å·®)")

    print("\nğŸ… --- B. Ranking Quality Metrics (é †åºã®æ­£ã—ã•) ---")
    # âœ… å¤‰æ›´ç®‡æ‰€: NDCG@Kã®çµæœã‚’ãƒ«ãƒ¼ãƒ—ã§è¡¨ç¤º
    print(f"   - **NDCG@K (æ­£è¦åŒ–å‰²å¼•ç´¯ç©åˆ©å¾—)**:")
    for k_str, score in ndcg_results.items():
        print(f"     - {k_str:<8}: {score:.4f}")
    print(f"     (äºˆæ¸¬ãƒªã‚¹ãƒˆã®ä¸Šä½Kä»¶ã«ãŠã‘ã‚‹é †åºã®æ­£ã—ã•ã€‚1ã«è¿‘ã„ã»ã©è‰¯ã„)")

    print(f"\n   - **RBP (ãƒ©ãƒ³ã‚¯ãƒã‚¤ã‚¢ã‚¹é©åˆç‡)**: {rbp_val:.4f}")
    print(f"     (ãƒ¦ãƒ¼ã‚¶ãŒãƒªã‚¹ãƒˆä¸Šä½ã‚’é‡è¦–ã™ã‚‹å‚¾å‘ã‚’è€ƒæ…®ã—ãŸé †åºã®æ­£ã—ã•)")


    end_time = time.time()
    print(f"\nTotal inference time: {end_time - start_time:.2f} seconds")


if __name__ == '__main__':
    # ----------------------------------------------------------------
    # --- å®Ÿè¡Œã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã‚’é¸æŠ ---
    # ----------------------------------------------------------------
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã•ã›ãŸã„å ´åˆã¯ã€ã“ã¡ã‚‰ã‚’å®Ÿè¡Œ
    # train_and_save_model()
    
    # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ï¼ˆäºˆæ¸¬ï¼‰ã ã‘ã‚’è¡Œã„ãŸã„å ´åˆã¯ã€ã“ã¡ã‚‰ã‚’ã‚³ãƒ¡ãƒ³ãƒˆè§£é™¤ã—ã¦å®Ÿè¡Œ
    run_inference()