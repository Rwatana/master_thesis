import os
import re
import torch
import numpy as np
import pandas as pd
from PIL import Image, ImageStat, ImageFile
from torchvision import models, transforms
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import multiprocessing

# ç”»åƒèª­ã¿è¾¼ã¿æ™‚ã®Truncatedã‚¨ãƒ©ãƒ¼ã‚’è¨±å®¹
ImageFile.LOAD_TRUNCATED_IMAGES = True

# --- è¨­å®š: Apple M1 Ultra (128GB RAM) æŽ¨å¥¨è¨­å®š ---
IMAGE_ROOT = os.path.expanduser('../image')
OUTPUT_FILE = 'image_features_v2_full_fixed_v2.csv'

# â˜… M1 Ultraã®ãƒ¡ãƒ¢ãƒª(128GB)ã‚’æ´»ã‹ã—ã¦å·¨å¤§ãƒãƒƒãƒã«ã™ã‚‹
# GPUä½¿ç”¨çŽ‡ã‚’100%ã«å¼µã‚Šä»˜ã‹ã›ã‚‹ãŸã‚ã®è¨­å®š
BATCH_SIZE = 1024 

# â˜… M1 Ultraã®é«˜æ€§èƒ½ã‚³ã‚¢æ•°(16)ã«åˆã‚ã›ã‚‹
# ã“ã‚Œä»¥ä¸Šå¢—ã‚„ã™ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¹ã‚¤ãƒƒãƒã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã§é€†ã«é…ããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
NUM_WORKERS = 20

# CSVæ›¸ãè¾¼ã¿é »åº¦ã‚’ä¸‹ã’ã‚‹ (ãƒ¡ãƒ¢ãƒªã«ä½™è£•ãŒã‚ã‚‹ãŸã‚)
WRITE_CHUNK_SIZE = 20000

# --- ãƒ‡ãƒã‚¤ã‚¹è¨­å®š (Apple Silicon Native) ---
# CUDAã§ã¯ãªãMPS (Metal Performance Shaders) ã‚’ä½¿ç”¨
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("âœ… Using Apple Metal Performance Shaders (MPS)")
else:
    device = torch.device("cpu")
    print("âš ï¸ MPS not available. Using CPU (Will be slow)")

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°: å‡¦ç†æ¸ˆã¿ãƒã‚§ãƒƒã‚¯ ---
def get_processed_post_ids(csv_path):
    if not os.path.exists(csv_path):
        return set()
    try:
        print(f"Loading existing IDs from {csv_path} for resume...")
        df = pd.read_csv(csv_path, usecols=['post_id'], dtype={'post_id': str})
        existing_ids = set(df['post_id'].values)
        print(f"-> Found {len(existing_ids)} processed images.")
        return existing_ids
    except Exception as e:
        print(f"Warning: Could not read existing file. Starting fresh. ({e})")
        return set()

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°: ç”»åƒçµ±è¨ˆé‡ (CPUå‡¦ç†) ---
def calculate_pixel_stats(image_pil):
    """
    CPUãƒ‘ãƒ¯ãƒ¼ã‚’ä½¿ã£ã¦è¨ˆç®—ã€‚M1 Ultraã®ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰æ€§èƒ½ãŒé«˜ã„ã®ã§é«˜é€Ÿã§ã™ã€‚
    """
    # 1. è¼åº¦ (Brightness)
    gray_img = image_pil.convert('L')
    stat = ImageStat.Stat(gray_img)
    brightness = stat.mean[0]

    # è¨ˆç®—ç”¨ã«numpyé…åˆ—åŒ–
    img_np = np.array(image_pil).astype(float)
    if len(img_np.shape) == 2:
        img_np = np.stack([img_np]*3, axis=-1)
    
    R, G, B = img_np[:,:,0], img_np[:,:,1], img_np[:,:,2]

    # 2. è‰²å½©åº¦ (Colorfulness)
    rg = np.abs(R - G)
    yb = np.abs(0.5 * (R + G) - B)
    std_root = np.sqrt(np.std(rg)**2 + np.std(yb)**2)
    mean_root = np.sqrt(np.mean(rg)**2 + np.mean(yb)**2)
    colorfulness = std_root + 0.3 * mean_root

    # 3. è‰²æ¸©åº¦ (Color Temperature)
    mean_R, mean_G, mean_B = np.mean(R), np.mean(G), np.mean(B)
    
    X = 0.4124 * mean_R + 0.3576 * mean_G + 0.1805 * mean_B
    Y = 0.2126 * mean_R + 0.7152 * mean_G + 0.0722 * mean_B
    Z = 0.0193 * mean_R + 0.1192 * mean_G + 0.9505 * mean_B
    
    if (X + Y + Z) == 0:
        cct = 0
    else:
        x = X / (X + Y + Z)
        y = Y / (X + Y + Z)
        denom = (0.1858 - y)
        if denom == 0:
             cct = 0
        else:
            n = (x - 0.3320) / denom
            cct = -449 * (n**3) + 3525 * (n**2) - 6823.3 * n + 5520.33

    return {
        'brightness': brightness,
        'colorfulness': colorfulness,
        'color_temp': cct
    }

# --- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®šç¾© ---
class InstagramImageDataset(Dataset):
    def __init__(self, root_dir, transform=None, processed_ids=None):
        self.root_dir = root_dir
        self.transform = transform
        self.image_files = []
        
        print("Scanning directory structure...")
        skip_count = 0
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªèµ°æŸ»
        for category in os.listdir(root_dir):
            cat_path = os.path.join(root_dir, category)
            if not os.path.isdir(cat_path) or category.startswith('.'): continue
            
            for username in os.listdir(cat_path):
                user_path = os.path.join(cat_path, username)
                if not os.path.isdir(user_path) or username.startswith('.'): continue
                
                for filename in os.listdir(user_path):
                    if filename.lower().endswith(('.jpg', '.jpeg', '.png')):
                        post_id = self._extract_post_id(filename)
                        
                        if processed_ids and post_id in processed_ids:
                            skip_count += 1
                            continue

                        self.image_files.append({
                            'path': os.path.join(user_path, filename),
                            'category': category,
                            'username': username,
                            'post_id': post_id,
                            'filename': filename
                        })
        
        print(f"Found {len(self.image_files) + skip_count} total images.")
        if skip_count > 0:
            print(f"Skipping {skip_count} processed images. Remaining: {len(self.image_files)}")

    def _extract_post_id(self, filename):
        base = os.path.splitext(filename)[0]
        if '-' in base:
            return base.split('-')[-1]
        return base

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        info = self.image_files[idx].copy()
        
        try:
            image = Image.open(info['path']).convert('RGB')
            
            # CPUä¸¦åˆ—å‡¦ç† (M1 Ultraã®20ã‚³ã‚¢ã‚’æ´»ç”¨)
            stats = calculate_pixel_stats(image)
            info.update(stats)
            
            if self.transform:
                image_tensor = self.transform(image)
                
            return image_tensor, info
        except Exception:
            return None, info

def collate_fn(batch):
    batch = [item for item in batch if item[0] is not None]
    if not batch: return None, None
    images = torch.stack([item[0] for item in batch])
    metadata = [item[1] for item in batch]
    return images, metadata

def flush_results_to_csv(results_buffer, output_file):
    if not results_buffer: return []
    df = pd.DataFrame(results_buffer)
    df['post_id'] = df['post_id'].astype(str)
    header = not os.path.exists(output_file)
    df.to_csv(output_file, mode='a', index=False, header=header)
    del df
    return []

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def main():
    # 1. Resumeç”¨ãƒã‚§ãƒƒã‚¯
    processed_ids = get_processed_post_ids(OUTPUT_FILE)
    
    # 2. ãƒ¢ãƒ‡ãƒ«æº–å‚™
    print("Loading ResNet50 for Metal (MPS)...")
    weights = models.ResNet50_Weights.DEFAULT
    model = models.resnet50(weights=weights)
    transform = weights.transforms()
    class_names = weights.meta["categories"]
    
    # M1 Ultraã¯å˜ä½“GPUã¨ã—ã¦èªè­˜ã•ã‚Œã‚‹ãŸã‚DataParallelã¯ä¸è¦
    model = model.to(device)
    model.eval()

    # 3. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼æº–å‚™
    dataset = InstagramImageDataset(IMAGE_ROOT, transform=transform, processed_ids=processed_ids)
    if len(dataset) == 0:
        print("No new images to process.")
        return

    dataloader = DataLoader(
        dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=False, 
        num_workers=NUM_WORKERS, 
        collate_fn=collate_fn,
        pin_memory=True,     # MPSã§ã‚‚æœ‰åŠ¹
        persistent_workers=True, # â˜…é‡è¦: macOSã§ã®å†spawnã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’é˜²ã
        prefetch_factor=2    # â˜…é‡è¦: æ¬¡ã®ãƒãƒƒãƒã‚’å¸¸ã«æº–å‚™ã•ã›ã¦CPUã‚’ä¼‘ã¾ã›ãªã„
    )

    # 4. ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œ
    results_buffer = []
    print(f"ðŸš€ Starting extraction on M1 Ultra (Batch: {BATCH_SIZE}, Workers: {NUM_WORKERS})...")
    
    with torch.no_grad():
        for images, metadata_list in tqdm(dataloader, desc="Extracting"):
            if images is None: continue
            
            images = images.to(device)
            
            # --- æŽ¨è«– (MPS) ---
            outputs = model(images)
            probs = torch.nn.functional.softmax(outputs, dim=1)
            top_probs, top_idxs = torch.max(probs, dim=1)
            
            # CPUã¸æˆ»ã™ (ã“ã“ãŒUnified Memoryã§é«˜é€Ÿ)
            top_probs = top_probs.cpu().numpy()
            top_idxs = top_idxs.cpu().numpy()
            
            # --- çµæžœæ ¼ç´ ---
            for i, meta in enumerate(metadata_list):
                row = {
                    'post_id': str(meta['post_id']),
                    'username': meta['username'],
                    'image_category': meta['category'],
                    'detected_object': class_names[top_idxs[i]],
                    'detection_confidence': top_probs[i],
                    'brightness': meta['brightness'],
                    'colorfulness': meta['colorfulness'],
                    'color_temp': meta['color_temp'],
                    'filename': meta['filename']
                }
                results_buffer.append(row)
            
            # --- ãƒãƒ£ãƒ³ã‚¯æ›¸ãè¾¼ã¿ ---
            if len(results_buffer) >= WRITE_CHUNK_SIZE:
                results_buffer = flush_results_to_csv(results_buffer, OUTPUT_FILE)
    
    if results_buffer:
        flush_results_to_csv(results_buffer, OUTPUT_FILE)
    
    print(f"\nDone! All data saved to: {OUTPUT_FILE}")

if __name__ == '__main__':
    multiprocessing.freeze_support()
    main()