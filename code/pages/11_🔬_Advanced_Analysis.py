import streamlit as st
import pandas as pd
import os
import json
from datetime import datetime, timedelta
import re
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import numpy as np

st.set_page_config(page_title="é«˜åº¦ãªæˆ¦ç•¥åˆ†æ", layout="wide")

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»è¨ˆç®—é–¢æ•° ---
@st.cache_data
def load_influencer_data(filepath):
    try:
        return pd.read_csv(filepath, sep='\t', skiprows=[1])
    except FileNotFoundError:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« '{filepath}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

@st.cache_data
def load_post_data(filepath):
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return None

def get_all_posts_with_details(username, info_dir):
    """é¸æŠã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨æŠ•ç¨¿ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹"""
    posts_details = []
    try:
        user_post_files = [f for f in os.listdir(info_dir) if f.startswith(f"{username}-")]
        for filename in user_post_files:
            data = load_post_data(os.path.join(info_dir, filename))
            if data:
                caption_edges = data.get('edge_media_to_caption', {}).get('edges', [])
                caption = caption_edges[0]['node']['text'] if caption_edges else ""
                posts_details.append({
                    'datetime': datetime.fromtimestamp(data.get('taken_at_timestamp', 0)),
                    'likes': data.get('edge_media_preview_like', {}).get('count', 0),
                    'comments': data.get('edge_media_to_parent_comment', {}).get('count', 0),
                    'caption': caption,
                    'caption_length': len(caption),
                    'has_question': '?' in caption
                })
        return pd.DataFrame(posts_details).sort_values('datetime').reset_index(drop=True)
    except FileNotFoundError:
        st.error(f"æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª `{info_dir}` ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return pd.DataFrame()

def find_growth_breakpoint(df, metric_col):
    if len(df) < 10: return None
    max_increase, breakpoint_idx = -np.inf, None
    for i in range(int(len(df) * 0.1), int(len(df) * 0.9)):
        increase = df[metric_col][i:].mean() - df[metric_col][:i].mean()
        if increase > max_increase:
            max_increase, breakpoint_idx = increase, i
    return df.index[breakpoint_idx] if breakpoint_idx is not None else None

def generate_wordcloud(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹"""
    if not text or text.isspace():
        return None
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    fig, ax = plt.subplots()
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    return fig

# --- UIæç”» ---
st.title("ğŸ”¬ é«˜åº¦ãªæˆ¦ç•¥åˆ†æ")
st.info("ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãŒæˆé•·ã—ãŸè¦å› ã‚’**ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æˆ¦ç•¥ã®å¤‰åŒ–**ã‹ã‚‰åˆ†æã—ã¾ã™ã€‚")

# --- ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ ---
df_influencers = load_influencer_data('influencers.txt')
info_dir = 'posts_info/unzipped_data_7z/info/'
if df_influencers is None:
    st.stop()

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
st.sidebar.header("åˆ†æå¯¾è±¡ã®é¸æŠ")
user_list = sorted(df_influencers['Username'].unique())
selected_user = st.sidebar.selectbox("åˆ†æã—ãŸã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’é¸æŠ:", options=user_list)

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
with st.spinner(f"{selected_user}ã®å…¨æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’è§£æä¸­..."):
    user_posts_df = get_all_posts_with_details(selected_user, info_dir)

if user_posts_df.empty:
    st.warning("ã“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    st.stop()

breakpoint_idx = find_growth_breakpoint(user_posts_df, 'likes')
if not breakpoint_idx:
    st.warning("æ˜ç¢ºãªæˆé•·ã®è»¢æ›ç‚¹ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    st.stop()

breakpoint_date = user_posts_df.loc[breakpoint_idx, 'datetime']
before_df = user_posts_df[user_posts_df.index < breakpoint_idx]
after_df = user_posts_df[user_posts_df.index >= breakpoint_idx]

st.success(f"æˆé•·ã®è»¢æ›ç‚¹ã‚’ **{breakpoint_date.strftime('%Y-%m-%d')}** ã¨æ¨å®šã—ã¾ã—ãŸã€‚")
st.markdown("---")

# --- 1. æˆ¦ç•¥å¤‰åŒ–ã®ã‚µãƒãƒªãƒ¼ ---
st.header("ğŸ“Š æˆ¦ç•¥å¤‰åŒ–ã®ã‚µãƒãƒªãƒ¼")
if not before_df.empty and not after_df.empty:
    # æŠ•ç¨¿é »åº¦ã®è¨ˆç®—
    days_before = (before_df['datetime'].max() - before_df['datetime'].min()).days + 1
    days_after = (after_df['datetime'].max() - after_df['datetime'].min()).days + 1
    freq_before = (len(before_df) / days_before) * 7 if days_before > 0 else 0
    freq_after = (len(after_df) / days_after) * 7 if days_after > 0 else 0

    # ãã®ä»–ã®æŒ‡æ¨™
    avg_len_before = before_df['caption_length'].mean()
    avg_len_after = after_df['caption_length'].mean()
    question_rate_before = before_df['has_question'].mean() * 100
    question_rate_after = after_df['has_question'].mean() * 100

    col1, col2, col3 = st.columns(3)
    col1.metric("é€±ã‚ãŸã‚Šã®å¹³å‡æŠ•ç¨¿æ•°", f"{freq_after:.2f} å›", f"{freq_after - freq_before:.2f} å›")
    col2.metric("å¹³å‡ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³é•·", f"{avg_len_after:.0f} æ–‡å­—", f"{avg_len_after - avg_len_before:.0f} æ–‡å­—")
    col3.metric("ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã§è³ªå•ã™ã‚‹å‰²åˆ", f"{question_rate_after:.1f} %", f"{question_rate_after - question_rate_before:.1f} %")
else:
    st.info("æ¯”è¼ƒã®ãŸã‚ã®ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")


# --- 2. ç™ºä¿¡ãƒ†ãƒ¼ãƒã®å¤‰åŒ–ï¼ˆãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ï¼‰ ---
st.markdown("---")
st.header("ğŸ¨ ç™ºä¿¡ãƒ†ãƒ¼ãƒã®å¤‰åŒ–")
st.write("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã§ã€æˆé•·ã®å‰å¾Œã§ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã§ä½¿ã‚ã‚Œã‚‹å˜èªãŒã©ã†å¤‰åŒ–ã—ãŸã‹ã‚’ç¤ºã—ã¾ã™ã€‚")

col_wc1, col_wc2 = st.columns(2)
with col_wc1:
    st.subheader("BEFORE (æˆé•·å‰)")
    text_before = " ".join(cap for cap in before_df['caption'])
    fig_before = generate_wordcloud(text_before)
    if fig_before:
        st.pyplot(fig_before)
    else:
        st.write("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

with col_wc2:
    st.subheader("AFTER (æˆé•·å¾Œ)")
    text_after = " ".join(cap for cap in after_df['caption'])
    fig_after = generate_wordcloud(text_after)
    if fig_after:
        st.pyplot(fig_after)
    else:
        st.write("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
